import cv2
import numpy as np

# Load the videos
video1_path = "sample-1.mp4"  # Update with correct path
video2_path = "sample-2.mp4"  # Update with correct path
cap1 = cv2.VideoCapture(video1_path)
cap2 = cv2.VideoCapture(video2_path)

if not cap1.isOpened():
    print("❌ Error: Could not open Video 1.")
if not cap2.isOpened():
    print("❌ Error: Could not open Video 2.")

# Initialize YOLO model for person detection
yolo_weights = "yolov3.weights"
yolo_config = "yolov3.cfg"
net = cv2.dnn.readNet(yolo_weights, yolo_config)
layer_names = net.getLayerNames()
output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]

# Load the reference image
reference_image = cv2.imread("reference.jpg")
if reference_image is None:
    print("❌ Error: Could not load reference image.")
    exit()

# Function to detect persons using YOLO
def detect_persons(frame):
    height, width = frame.shape[:2]
    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)
    net.setInput(blob)
    detections = net.forward(output_layers)
    
    boxes = []
    confidences = []
    
    for detection in detections:
        for obj in detection:
            scores = obj[5:]
            class_id = np.argmax(scores)
            confidence = scores[class_id]
            
            if class_id == 0 and confidence > 0.5:  # Class 0 = 'person'
                center_x, center_y, w, h = (obj[:4] * np.array([width, height, width, height])).astype("int")
                x, y = int(center_x - w / 2), int(center_y - h / 2)
                boxes.append([x, y, w, h])
                confidences.append(float(confidence))

    return boxes, confidences

# Initialize tracking
tracker = cv2.TrackerCSRT_create()  # Change to TrackerKCF if CSRT doesn't work
tracking = False
track_box = None

# Process each frame from Video 1
while cap1.isOpened():
    ret, frame = cap1.read()
    if not ret:
        print("End of Video 1")
        break
    
    if not tracking:
        # Detect persons
        boxes, confidences = detect_persons(frame)
        
        # Select the first detected person as the reference
        if boxes:
            track_box = tuple(boxes[0])
            tracker.init(frame, track_box)
            tracking = True

    else:
        # Track the person
        success, track_box = tracker.update(frame)
        if success:
            x, y, w, h = [int(i) for i in track_box]
            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)
            cv2.putText(frame, "The_Target", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)

    cv2.imshow("Tracking - Video 1", frame)
    
    # If 'q' is pressed, break the loop and switch to Video 2
    if cv2.waitKey(1) & 0xFF == ord("q"):
        print("Switching to Video 2...")
        break

# Now process each frame from Video 2
while cap2.isOpened():
    ret, frame = cap2.read()
    if not ret:
        print("End of Video 2")
        break

    # Track the person in Video 2
    success, track_box = tracker.update(frame)
    if success:
        x, y, w, h = [int(i) for i in track_box]
        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)
        cv2.putText(frame, "The_Target", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)

    cv2.imshow("Tracking - Video 2", frame)
    
    # If 'q' is pressed, break the loop and stop processing
    if cv2.waitKey(1) & 0xFF == ord("q"):
        break

# Release resources
cap1.release()
cap2.release()
cv2.destroyAllWindows()
